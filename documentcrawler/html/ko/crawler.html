<h1>크롤러(1) - A 위치에서 B 위치로 데이터 이동을 위한 크롤러</h1><h2>시놉시스</h2><p>사용법: crawler [ crawl | testit | restart | resume | refresh | summary ][ options ]</p><h2>설명</h2><p>Data Crawler는 컨텐츠 관리 시스템 및 파일 시스템 등과 같은 데이터의 다양한 저장소를 크롤링한 후에 결과 문서를 원격 서비스로 푸시하는 데 사용됩니다. </p><h2>글로벌 옵션</h2>
<pre><code>--version
    프로그램 버전을 표시합니다.
--help
    이 사용법 텍스트를 표시합니다. 
</code></pre><h2>명령</h2><h3>crawl [ options ]</h3><p>현재 구성으로 크롤링을 실행합니다. </p>
<pre><code>-c &lt;value&gt; | --config &lt;value&gt;  # 사용할 구성 파일을 지정합니다. 기본값은 &quot;config/crawler.conf&quot;입니다.
--pii-checking &lt;value&gt;         # PII 확인을 토글합니다. 
</code></pre><h3>testit [ options ]</h3><p>시드 URL만 크롤링하며 인큐된 URL을 표시하는 테스트 크롤링을 실행합니다. 시드 URL이 결과적으로<br/>색인화 가능 컨텐츠(예: 문서)를 생성하는 경우, 해당 컨텐츠는 출력 어댑터로 전송되며 컨텐츠가 화면에 인쇄됩니다.<br/>시드 URL 검색으로 URL이 인큐되면 해당 URL이 표시되며, 어떤 컨텐츠도 출력 어댑터로 전송되지 않습니다. 기본적으로 5개의 인큐된 URL이 표시됩니다. </p>
<pre><code>-c &lt;value&gt; | --config &lt;value&gt;  # 사용할 구성 파일을 지정합니다. 기본값은 &quot;config/crawler.conf&quot;입니다.
-l &lt;n&gt; | --limit &lt;n&gt;           # 표시되는 인큐된 URL의 수를 제한합니다.
--pii-checking &lt;value&gt;         # PII 확인을 토글합니다. 
</code></pre><h3>restart [ options ]</h3><p>크롤링 재시작을 실행합니다. 현재 구성으로 새 크롤링을 실행합니다. </p>
<pre><code>-c &lt;value&gt; | --config &lt;value&gt;  # 사용할 구성 파일을 지정합니다.
--pii-checking &lt;value&gt;         # PII 확인을 토글합니다. 
</code></pre><h3>resume [ options ]</h3><p>중지된 위치에서 크롤링을 재개합니다. </p>
<pre><code>-c &lt;value&gt; | --config &lt;value&gt;  # 사용할 구성 파일을 지정합니다.
--pii-checking &lt;value&gt;         # PII 확인을 토글합니다. 
</code></pre><h3>refresh [ options ]</h3><p>이전 크롤링을 새로 고칩니다. </p>
<pre><code>-c &lt;value&gt; | --config &lt;value&gt;  # 사용할 구성 파일을 지정합니다.
--pii-checking &lt;value&gt;         # PII 확인을 토글합니다. 
</code></pre><h3>summary [ options ]</h3><p>크롤링 보고서를 생성합니다. </p>
<pre><code>--submitted                    # 제출된 모든 문서를 조회합니다.
--processed                    # 정상적으로 처리된 문서만 조회합니다.
--failed                       # 정상 처리에 실패한 문서만 조회합니다.
--group-id &lt;value&gt;             # 지정된 그룹에 대한 크롤링 실행을 조회합니다. 그룹은 초기 크롤링 및 해당 초기 크롤링의 재개, 새로 고치기 또는 재시작으로 구성됩니다. 값이 지정되지 않은 경우, 이 조회의 기본값은 크롤링된 최신 그룹입니다.
--show-content                 # 조회와 연관된 추가 컨텐츠를 표시합니다.
--filter                       # URL 및 hashID의 조회 결과를 필터링합니다. 
</code></pre><h2>예제</h2><p><code>config/crawler.conf</code>에서 구성 파일을 사용하여 크롤링을 실행합니다. </p>
<pre><code>crawler crawl
</code></pre><p><code>config/crawler.conf</code>에서 구성 파일을 사용하여 테스트를 실행합니다. </p>
<pre><code>crawler testit
</code></pre><p><code>/home/watson/office-share.conf</code>에서 구성 파일을 사용하여 크롤링을 실행합니다. </p>
<pre><code>crawler crawl --config /home/watson/office-share.conf
</code></pre><p><code>/home/watson/office-share.conf</code>에서 구성 파일을 사용하여 크롤링을 다시 시작합니다. </p>
<pre><code>crawler restart --config /home/watson/office-share.conf
</code></pre><p><code>2</code>의 그룹 ID로 실패한 문서에 대한 요약 정보를 가져옵니다. </p>
<pre><code>crawler summary --failed --group-id 2 --show-content
</code></pre><p>버전을 포함하여 사용법을 표시합니다. </p>
<pre><code>crawler --help
</code></pre><h2>구성</h2><p><code>crawler</code>에서는 해당 옵션에 대한 구성 파일이 필요합니다. 구성 파일의 예제는<br/><code>crawler</code> 설치 디렉토리 내의 <code>share</code> 디렉토리에서 제공됩니다.<br/>이러한 예제를 복사하고 이를 수정하십시오. *해당 위치에서는 예제를 수정하지 마십시오. *</p><p><code>--config | -c</code> 옵션을 지정하지 않은 경우, <code>crawler</code>는<br/><code>crawler</code>가 시작된 디렉토리의 <code>config</code> 디렉토리에서 해당 구성을 찾습니다.<br/>즉, <code>crawler</code>는 <code>config/crawler.conf</code>를 찾습니다. </p><h2>진단</h2><p>이 기능을 사용하여 문제점을 진단할 수 있습니다. </p><h3>디버깅</h3><p>디버깅 모드를 활성화합니다. <code>crawler.conf</code> 파일에서 다음을 설정하십시오. </p>
<pre><code>debugging.full_node_debugging = true
</code></pre><h3>로깅</h3><p>로깅을 사용합니다. <code>log4j_custom.properties</code> 파일에서 다음을 설정하십시오. </p>
<pre><code>log4j.rootLogger=INFO, Console, Log
</code></pre><p>이는 파일 출력에 대한 기본 로깅 레벨입니다. 콘솔 로그의 경우, 기본값은 다음과 같습니다. </p>
<pre><code>log4j.appender.Console.Threshold=WARN
</code></pre><p>로깅 레벨은 다음 값으로 설정될 수 있습니다. </p>
<pre><code>OFF - 최상위의 가능한 순위이며, 이는 로깅을 끄는 용도로 사용됩니다.
FATAL - 애플리케이션 중단을 유발할 가능성이 큰 매우 심각한 오류 이벤트를 지정합니다.
ERROR - 아직은 애플리케이션이 실행을 계속하도록 허용할 수 있는 오류 이벤트를 지정합니다.
WARN - 잠재적으로 유해한 상황을 지정합니다.
INFO - 상세하지 않은 레벨에서 애플리케이션의 진행 상태를 강조표시하는 정보 메시지를 지정합니다.
DEBUG - 애플리케이션을 디버그하기에 최적인 상세한 정보 이벤트를 지정합니다.
TRACE - DEBUG보다 상세한 정보 이벤트를 지정합니다.
ALL - 최하위의 가능한 순위이며, 이는 모든 로깅을 켜는 용도로 사용됩니다. 
</code></pre><h2>제한</h2><p>처리량을 관리하는 데 도움이 되도록 크기 제한사항을 정의합니다. <code>crawler.conf</code> 파일에서 다음을 설정하십시오. </p><p><code>shutdown_timeout</code> 애플리케이션을 종료하기 전의 제한시간 값(분 단위)을 지정합니다. 기본값은 10입니다. </p>
<pre><code>shutdown_timeout = &lt;n&gt;
</code></pre><p><code>output_limit</code>는 휴대용 크롤러가 리턴을 대기하며 출력 어댑터에 동시에 전송할<br/>색인화 가능 항목의 최대 수입니다. 기본값은 10입니다. 이는 작업 수행에 사용 가능한 코어에 의해 추가로 제한될 수 있습니다. </p>
<pre><code>output_limit = &lt;n&gt;
</code></pre><p><code>input_limit</code> 동시에 커넥터에서 요청 가능한 URL의 수를 제한합니다. 기본값은 3입니다. </p>
<pre><code>input_limit = &lt;n&gt;
</code></pre><p><code>output_timeout</code>은 휴대용 크롤러가 출력 어댑터에 대한 요청을 포기한 후에<br/>한계 큐에서 해당 항목을 제거하여 추가 처리를 허용하기 전의 시간의 양(초 단위)입니다. 기본값은 150입니다.</p>
<pre><code>output_timeout = &lt;n&gt;
</code></pre><p>해당 제한조건이 여기에 정의된 한계와 관련될 수 있으므로, 출력 어댑터에 의해 적용되는 제한조건에 대해서는 고려가 필요합니다.<br/>위에서 정의된 <code>output_limit</code>는 오직 동시에 출력 어댑터에 전송될 수 있는 색인화 가능 오브젝트의 수에만 관련됩니다.<br/>일단 출력 어댑터로 전송된 경우, 색인화 가능 오브젝트는 <code>output_timeout</code> 변수에서 정의한 대로 &ldquo;클럭 상&rdquo;에 놓입니다.<br/>출력 어댑터 자체에 제한이 있어서 수신하는 수 만큼의 입력을 처리하지 못할 수 있습니다.<br/>예를 들어, 오케스트레이션 출력 어댑터에는 서비스에 대한 HTTP 연결을 위해 구성 가능한 연결 풀이 있을 수 있습니다.<br/>예를 들어, 해당 기본값이 8이며 해당 연결 풀에 대해 구성된 것보다 큰 수로 <code>output_limit</code>를 설정하면<br/>실행할 차례를 대기하는 클럭 상의 프로세스를 보유하게 됩니다. 그러면 제한시간 초과가 발생할 수 있습니다. </p><p><code>num_threads</code> 동시에 실행 가능한 병렬 스레드의 수입니다.<br/>이 값은 직접 병렬 스레드의 수를 지정하는 정수이거나,<br/>사용 가능한 프로세서의 수의 증배율을 지정하는 <code>&quot;xNUM&quot;</code> 형식의 문자열일 수 있습니다.<br/>기본값은 &ldquo;x1.5&rdquo; 또는 사용 가능한 프로세서의 수의 1.5배입니다(<code>Runtime.availableProcessors</code>에서 취한 대로). </p>
<pre><code>num_threads = &lt;n&gt;
</code></pre><p>Data Crawler 풀에서 병렬성을 계산하는 공식은 <code>min(maxThreads, max(minThreads, numThreads))</code>입니다. </p><h2>환경 변수 <code>CRAWLER_OPTS</code></h2><p>다음은 기본값으로 나열된 <code>CRAWLER_OPTS</code> 환경 변수를 통해 <code>crawler</code>에 전달될 수 있는 특성입니다. </p><p>다음과 같이 이를 전달하십시오. </p>
<pre><code>CRAWLER_OPTS=&quot;-Dproperty=value -Dproperty=value&quot; crawler
</code></pre><p>이는 디버깅 용도로만, 그리고 IBM 지원 센터의 지시 하에서만 변경되어야 합니다. </p><h3>cfa.java_bin</h3><p><code>cfa.java_bin</code>은 커넥터 프레임워크 입력 어댑터를 시작하는 데 사용되는<br/><code>java</code> 명령을 변경할 수 있습니다. 기본적으로, <code>crawler</code>는<br/><code>crawler</code> 자체를 실행하는 데 사용되는 것과 동일한 <code>java</code> 2진을 사용합니다. </p><p>또한 이는 <code>java.home</code> 특성을 설정하여 변경이 가능하며, 이는 다시 <code>java</code> 실행 파일에 대한 경로를 계산하는 데 사용됩니다. </p><h3>cfa.lib_dir</h3><p><code>cfa.lib_dir</code>은 커넥터 프레임워크의 <code>lib</code> 디렉토리에 대한 경로를 변경합니다. 가급적이면 이를 변경하지 마십시오.<br/>기본적으로, <code>crawler</code>는 커넥터 프레임워크(보통 단순히 <code>connectorFramework</code>)에 대한 계산된 경로 내의 <code>lib</code> 디렉토리를 사용합니다. </p><h3>cfa.framework_jars_dir</h3><p><code>cfa.framework_jars_dir</code>은 기본적으로 <code>connectorFramework/&lt;version&gt;/lib/java</code>에 있는 커넥터 프레임워크의 jars 디렉토리로 경로를 변경합니다. </p><h3>cfa.plugins_dir</h3><p><code>cfa.plugins_dir</code>은 실제 커넥터가 저장된 커넥터 프레임워크의 플러그인 디렉토리로 경로를 지정합니다.<br/>기본적으로, 이는 <code>framework_jars_dir</code>에서 빌드되며 <code>connectorFramework/&lt;version&gt;/lib/java/plugins</code>가 됩니다. </p><h2>알려진 제한사항</h2><p>Data Crawler의 현재 릴리스에서 알려진 제한사항을 자세히 설명합니다. </p>
<ul>
  <li>올바르지 않거나 누락된 URL로 파일 시스템 커넥터를 실행하면 Data Crawler가 정지될 수 있습니다.</li>
  <li>모든 화이트리스트 URL 또는 RegExe가 단일 RegEx 표현식에 포함되도록 <code>config/crawler.conf</code> 파일에서 <code>urls_to_filter</code> 값을 구성하십시오.</li>
  <li><code>--config | -c</code> 옵션에서 전달된 구성 파일에 대한 경로는 규정된 경로여야 합니다. 즉, 이는 상대 형식 <code>config/crawler.conf</code> 또는 <code>./crawler.conf</code>이거나 절대 경로 <code>/path/to/config/crawler.conf</code>여야 합니다. 단순 <code>crawler.conf</code> 지정은 <code>crawler.conf</code> 파일에서 <code>include</code>를 사용하여 참조된 파일이 <code>include</code>를 사용하는 대신 인라인된 경우에만 가능합니다. 예를 들어, <code>discovery/discovery_service.conf</code>는 구성의 읽기가 쉬워지도록 <code>include</code>됩니다. 해당 컨텐츠는 config 옵션에서 규정되지 않은 경로를 사용할 수 있도록 <code>output_adapter.discovery_service</code> 키 내에서 <code>crawler.conf</code>로 복사되어야 합니다.</li>
</ul><h2>변경 로그</h2><p>이 릴리스에서 변경사항의 목록은 설치 디렉토리의 <code>changelog.txt</code> 파일을 참조하십시오. </p><h2>작성자</h2><p>IBM Watson - <a href="https://www.ibm.com/smarterplanet/us/en/ibmwatson/">https://www.ibm.com/smarterplanet/us/en/ibmwatson/</a></p><p>피츠버그에서 yinz에 의해 작성되었습니다. </p><h2>관련 항목</h2><p>vcrypt(1)</p><p>crawler.conf(5)</p><p>crawler-options.conf(5)</p><p>crawler-seed.conf(5)</p><p>orchestration_service.conf(5)</p>