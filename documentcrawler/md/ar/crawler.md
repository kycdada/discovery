‏‎crawler(1)‎‏ - أداة بحث لنقل البيانات من النقطة A الى النقطة B
====================================================================

## المختصر

الاستخدام: أداة بحث [ بحث صفحات الانترنت | TestIt | اعادة البدء | الاستئناف | التجديد | الملخص ][ options ]

## الوصف

يستخدم Data Crawler لبحث مستودعات تخزين بيانات متنوعة، على سبيل المثال أنظمة ادارة المحتويات وأنظمة الملفات ثم دفع الوثائق الناتجة الى خدمة عن بعد.

## اختيارات شاملة

    --النسخة
        يقوم بعرض نسخ البرنامج
    --المساعدة
        يقوم بعرض نص هذا الاستخدام

## الأوامر

### بحث صفحات الانترنت [ الاختيارات ]

يقوم بتشغيل بحث صفحات الانترنت باستخدام التوصيف الحالي.

    ‏‎-c <value> | --config <value>‎‏  # يقوم بتحديد ملف التوصيف ليتم استخدامه. المفترض هو ‏‎"config/crawler.conf"‎‏.
    --pii-checking <value>  # يقوم بتبديل فحص المعلومات الشخصية

### TestIt [ الاختيارات ]

يقوم بتشغيل اختبار لبحث صفحات الانترنت، والذي يقوم ببحث فقط عنوان URL أساس الترقيم الآلى ويعرض أية عناوين URL بالصف. اذا نتج عن عنوان URL أساس الترقيم الآلى محتويات قابلة للفهرسة (على سبيل المثال، انها وثيقة)، فسيتم ‏ارسال‏ تلك المحتويات الى موفق المخرجات ويتم طباعة المحتويات بالشاشة. اذا تسبب عنوان URL الخاص بأساس الترقيم الآلى في تعلق صفوف عناوين URL، فسيتم عرض تلك العناوين ولن يتم ارسال أية محتويات الى مخرجات الموفق. وبصفة مفترض، سيتم عرض خمسة عناوين URL معلقة في الصف.

    ‏‎-c <value> | --config <value>‎‏  # يقوم بتحديد ملف التوصيف ليتم استخدامه. المفترض هو ‏‎"config/crawler.conf"‎‏.
    -l <n> | --limit <n>           # يقوم بحد عدد عناوين URL المعلقة في الصف والتي يتم عرضها.
    --pii-checking <value>  # يقوم بتبديل فحص المعلومات الشخصية

### اعادة البدء [ الاختيارات ]

يقوم بتشغيل عملية اعادة بدء بحث صفحات الانترنت باستخدام التوصيف الحالي.

    ‏‎-c <value> | --config <value>‎‏  # يقوم بتحديد ملف التوصيف ليتم استخدامه.
    --pii-checking <value>  # يقوم بتبديل فحص المعلومات الشخصية

### الاستئناف [ الاختيارات ]

يقوم باستئناف بحث صفحات الانترنت من حيث توقف.

    ‏‎-c <value> | --config <value>‎‏  # يقوم بتحديد ملف التوصيف ليتم استخدامه.
    --pii-checking <value>  # يقوم بتبديل فحص المعلومات الشخصية

### التجديد [ الاختيارات ]

يقوم بتجديد بحث صفحات الانترنت السابق.

    ‏‎-c <value> | --config <value>‎‏  # يقوم بتحديد ملف التوصيف ليتم استخدامه.
    --pii-checking <value>  # يقوم بتبديل فحص المعلومات الشخصية

### الملخص [ الاختيارات ]

يقوم بتكوين تقرير عن بحث صفحات الانترنت.

    --تم احالتها           # يقوم بالاستعلام عن كل الوثائق التي تم احالتها
    --تم تشغيلها                   # يقوم بالاستعلام فقط عن كل الوثائق التي تم تشغيلها بنجاح
    --فشلت                         # يقوم بالاستعلام فقط عن كل الوثائق التي فشلت عملية تشغيلها بنجاح
    --group-id <value>       # يقوم بالاستعلام عن بحث صفحات الانترنت الذي يعمل لمجموعة محددة. أية مجموعة تتكون من بحث صفحات انترنت مبدئي، وأي استئناف، أو تجديد، أو اعادة بدء لبحث صفحات الانترنت المبدئي هذا. اذا لم يتم تحديد القيمة، فسيرجع هذا الاستعلام افتراضيا الة أحدث مجموعة تم بحث صفحات الانترنت لها.
    --عرض-المحتويات                # يقوم بعرض المحتويات المرفقة بالاستعلام
    --مرشح البيانات                # يقوم بترشيح نتيجة الاستعلام الي تم على عنوان URL وكود التجزئة

## أمثلة

تشغيل بحث صفحات الانترنت باستخدام ملف التوصيف الذي على ‏‎`config/crawler.conf`‎‏:

    بحث صفحات الانترنت الى crawler

تشغيل اختبار باستخدام ملف التوصيف الذي على ‏‎`config/crawler.conf`‎‏:

    اختبار crawler

تشغيل بحث صفحات الانترنت باستخدام ملف التوصيف الذي على ‏‎`/home/watson/office-share.conf`‎‏:

    بحث صفحات الانترنت بالنسبة الى crawler --توصيف ‏‎/home/watson/office-share.conf‎‏

اعادة بدء بحث صفحات الانترنت باستخدام ملف التوصيف الذي على ‏‎`/home/watson/office-share.conf`‎‏:

    اعادة بدء crawler --توصيف ‏‎/home/watson/office-share.conf‎‏

الحصول على على معلومات عن الوثائق التي فشلت بكود تعريف المجموعة ‏‎`2`‎‏:

    ملخص crawler --فشل --كود تعريف المجموعة 2 --عرض-المحتويات

عرض الاستخدام، بما في ذلك النسخة:

    crawler --المساعدة

## توصيف

`crawler` يتطلب ملف توصيف للاختيارات الخاصة به. تتوافر أمثلة على ملفات التوصيف في دليل `المشاركة` في دليل تركيب `crawler`. يمكن نسخ هذه الأمثلة وتعديلها. *لا تقوم بتعديل الأمثلة في مكانها.*

بدون تعديل الاختيار `--config | -c`، سيقوم`crawler` بالبحث عن التوصيف الخاص به في الدليل `config` من الدليل الذي بدأ فيه ‏‎`crawler`‎‏. وهو، قيام `crawler` بالبحث عن ‏‎`config/crawler.conf`‎‏.

## تشخيصات

استخدام هذه الخصائص لتشخيص المشكلات.

### تصحيح الأخطاء

تفعيل نمط تصحيح الأخطاء. في الملف ‏‎`crawler.conf`‎‏، قم بتحديد:

    debugging.full_node_debugging = true

### التسجيل

اتاحة التسجيل. في الملف ‏‎`log4j_custom.properties`‎‏، قم بتحديد:

    ‏‎log4j.rootLogger=INFO‎‏، شاشة التحكم الرئيسية، السجل

يعد هذا هو مستوى التسجيل المفترض لمخرجات الملف.  بالنسبة لسجل شاشة التحكم الرئيسية، المفترض هو:

    ‏‎log4j.appender.Console.Threshold=WARN‎‏

يمكن تحديد مستويات التسجيل بالقيم التالية:

    ايقاف - أعلى مرتبة ممكنة، والمقصود بها ايقاف التسجيل.
    جسيم - تحديد كل أحداث الخطأ الجسيمة التي يفترض أن تؤدي الى احباط التطبيق.
    خطأ - تحديد أحداث الخطأ التي لا زالت قد تمسح باستمرار تشغيل التطبيق.
    تحذير - تحديد الحالات التي من المحتمل أن تكون ضارة.
    معلومات - تحديد الرسائل الاعلامية التي تقوم باظهار تقدم التطبيق بمستوى لا يمكن التحكم فيه.
    تصحيح الأخطاء - تحديد الأحداث الاعلامية التي يمكن التحكم فيها والتي تعد كبيرة الفائدة لتصحيح أخطاء أحد التطبيقات.
    تتبع - تحديد الأحداث الاعلامية التي يمكن التحكم فيها بشكل أكبر بدال من تصحيح الأخطاء.
    الكل - أقل مرتبة ممكنة، والمقصود بها تشغيل كل التسجيلات.

## تضيق الاختيارات

تعريف حدود الحجم، للمساعدة في ادارة الانتاجية. في الملف ‏‎`crawler.conf`‎‏، قم بتحديد:

`shutdown_timeout` يقوم بتحديد قيمة انتهاء الوقت، بالدقائق، قبل اغلاق التطبيق؛ القيمة المفترضة هي 10.

    shutdown_timeout = <n>

`output_limit` هو أعلى عدد من البنود المفهرسة التي تستطيع أداة البحث المتنقلة ارساله في نفس الوقت الى موفق المخرجات، وفي انتظار رجوعهم؛ القيمة المفترضة هي 10‏. قد يتم حد هذا بشكل أكبر بواسطة الأجزاء المركزية المتاحة للقيام بالعمل.

    ‏‎output_limit = <n>‎‏

`input_limit` يقوم بحد عدد عناوين URL التي يمكن طلبها من الموصل في نفس الوقت؛ القيمة المفترضة هي 3.

    ‏‎input_limit = <n>‎‏

`output_timeout` هو مقدار الوقت، بالثانية، قبل تخلي أداة البحث المتنقلة عن ارسال طلب الى موفق المخرجات، ثم ازالة البند من صف الحد، للسماح بالمزيد من عمليات التشغيل. القيمة المفترضة هي 150‏.

    ‏‎output_timeout = <n>‎‏

يجب الاهتمام بالقيود التي يتم فرضها بواسطة موفق المخرجات حيث قد تتعلق تلك القيود بالحدود التي يتم تعريفها هنا. `output_limit` الذي تم تعريفه بأعلى يتعلق فقط بعدد العناصر القابلة للفهرسة التي يمكن ارسالها الى موفق المخرجات فورا. بمجرد ارسال عنصر قابل للفهرسة الى موفق المخرجات، يبدأ "العد الزمني،" كما هو معرف بواسطة المتغير `output_timeout`. من الممكن أن يتوافر لموفق المخرجات نفسه خاصية تضيق الاختيارات مما يمنعه من القدرة على تشغيل نفس عدد المدخلات الذي يقوم باستلامه. على سبيل المثال، قد يتوافر لموفق مخرجات التنسيق مستودع وصلات، قابل للتوصيف لاتصالات HTTP بالخدمة. اذا كان المفترض 8، على سبيل المثال، واذا تم اعداد `output_limit` ليكون رقم أكبر من العدد الذي تم توصيفه لمستودع الوصلات، فسيتوافر لديك عمليات، على مدار الساعة، في انتظار دورها ليتم تنفيذها. ويمكنك مواجهة حالات انتهاء الوقت.

`num_threads` عدد سلاسل العمليات المتوازية التي ي مكن تشغيلها في وقت واحد. يمكن أن تكون هذه القيمة اما رقم صحيح، وهي التي تقوم بتحديد عدد سلاسل العمليات المتوازية مباشرة، أو أن تكون مجموعة حروف، بالنسق `"xNUM"`، مما يحدد عامل الضرب لعدد وحدات التشغيل المتاحة. القيمة
المفترضة هي ‏‎"x1.5"‎‏، أو 1.5 ضعف عدد وحدات التشغيل المتاحة (كما تم أخذه باستخدام ‏‎`Runtime.availableProcessors`‎‏).

    ‏‎num_threads = <n>‎‏

المعادلة الخاصة باحتساب التوازي في مستودع تخزين Data Crawler هي: ‏‎`min(maxThreads, max(minThreads, numThreads))`‎‏.

## متغير بيئة التشغيل `CRAWLER_OPTS` 

فيما يلي خصائص يمكن تمريرها الى `crawler` بواسطة متغير بيئة التشغيل `CRAWLER_OPTS`، الذي يتم عرضه بالقيم المفترضة.

قم بتمريرهم على النحو التالي:

    CRAWLER_OPTS="-Dproperty=value -Dproperty=value" crawler

لا يجب تغييرهم الا لتصحيح أخطاء، وفقط تحت توجيهات دعم IBM.

### cfa.java_bin

‏‎`cfa.java_bin`‎‏ يستطيع تغيير أمر `java` المستخدم لبدء Connector Framework Input Adapter. بصفة مفترضة، يقوم `crawler` باستخدام نفس ثنائي `java` المستخدم لبدء تشغيل `crawler` نفسه.

يمكن تغيير ذلك أيضا بواسطة تحديد الخاصية ‏‎`java.home`‎‏، والتي سيتم استخدامها لاحتساب مسار امكانية التنفيذ `java`.

### ‏‎cfa.lib_dir‎‏

‏‎`cfa.lib_dir`‎‏ يقوم بتغيير مسار الى دليل ‏‎Connector Framework's `lib`‎‏. نادرا ما يجب تغيير هذا. بصفة مفترضة يقوم `crawler` باستخدام الدليل `lib` في داخل المسار الذي يتم احتسابه الى Connector Framework، عامة ببساطة `connectorFramework`.

### ‏‎cfa.framework_jars_dir‎‏

‏‎`cfa.framework_jars_dir`‎‏ يقوم بتغيير المسار الخاص بدليل ملف تخزين Java بالنسبة الى Connector Framework، والذي هو، بصفة مفترضة، في ‏‎`connectorFramework/<version>/lib/java`‎‏.

### ‏‎cfa.plugins_dir‎‏

‏‎`cfa.plugins_dir`‎‏ يقوم بتحديد مسار دليل البرامج المساعدة الى Connector Framework، حيث يتم تخزين الموصلات الفعلية.
بصفة مفترضة، يتم بناء هذا من ‏‎`framework_jars_dir`‎‏ وسيصبح ‏‎`connectorFramework/<version>/lib/java/plugins`‎‏.

## الحدود المعروفة

يقوم بعرض تفاصيل الحدود المعروفة في الاصدار الحالي من Data Crawler

* قد يصبح Data Crawler معلق عند تشغيل موصل Filesystem باستخدام عنوان URL غير صحيح أو غير موجود.
* يمكن توصيف قيمة `urls_to_filter` في الملف ‏‎`config/crawler.conf`‎‏ بحيث يتم تضمين كل عناوين URL التي في الكشف المسموح به أو التعبيرات RegEx.
* يجب أن يكون مسار ملف التوصيف الذي تم تمريره في الاختيار ‏‎`--config | -c`‎‏ هو مسار مصنف. وهو، أن يكون في النسق المناسب ‏‎`config/crawler.conf`‎‏، أو ‏‎`./crawler.conf`‎‏، أو المسار المطلق ‏‎`/path/to/config/crawler.conf`‎‏. يمكن تحديد ‏‎`crawler.conf`‎‏ فقط اذا تم ضمين الملفات المشار اليه باستخدام `include` في الملف ‏‎`crawler.conf`‎‏ بدلا من استخدام ‏‎`include`‎‏. على سبيل المثال، ‏‎`discovery/discovery_service.conf`‎‏ كان `include` لجعل التوصيف أسهل للقراءة. ويجب نسخ المحتويات الخاصة به الى ‏‎`crawler.conf`‎‏ في المفتاح ‏‎`output_adapter.discovery_service`‎‏ لتتمكن من استخدام مسار غير مصنف في اختيار التوصيف config.

## تغيير السجل

ارجع الى الملف ‏‎`changelog.txt`‎‏ الذي في دليل التركيب للحصول على كشف بالتغييرات التي في هذا الاصدار.

## المؤلف

IBM Watson - ‏‎https://www.ibm.com/smarterplanet/us/en/ibmwatson/‎‏

صنع بواسطة yinz في بيتسبرج.

## يمكنك الاطلاع أيضا على

‏‎vcrypt(1)‎‏

‏‎crawler.conf(5)‎‏

‏‎crawler-options.conf(5)‎‏

‏‎crawler-seed.conf(5)‎‏

‏‎orchestration_service.conf(5)‎‏
