# All file paths are relative to the directory containing this file, except where noted.
# Not all values are required; those values which are not default to what is in this example configuration.
input_adapter {
    # The Crawl class for the chosen input adapter.
    # The only input adapter that exists is the default value for this.
    class = "com.ibm.watson.crawler.connectorframeworkinputadapter.Crawl",

    # The configuration key within this block to pass to the chosen input adapter.
    config = "connector_framework",

    # The configuration for crawler state storage
    storage = {
       location = "./storage/crawler",
       # Username for the database
       username = "crawluser",
       # Password for the database, if any.
       password = "",
       # cache_size in kb that H2 uses for caching the database in memory.
       # The recommended value is 64kb for each GB of memory avaialble to
       # the JVM.
       cache_size = 1048576
    },

    # The configuration for the Connector Framework.
    connector_framework = {
      # The file containing the connector configuration for the crawl.
      crawl_config_file = "connectors/database.conf",

      # The file containing the seed URL for the crawl.
      crawl_seed_file = "seeds/database-seed.conf",

      # The file containing the encryption key used for decrypting passwords.
      # Generate a new key with the `vcrypt` command included with Crawler.
      id_vcrypt_file = "id_vcrypt",

      # Crawler temp folder for connector logs, download cache, and connector unpacking
      crawler_temp_dir = "tmp",

      # Adds a directory of jars to the Connector Framework classpath.
      # This is relative to connectorFramework/lib/java directory off of the installation directory.
      # Supply "oakland" when using the SharePoint connector.
      # Supply "database" when using the Database connector.
      extra_jars_dir = "database",

      # Crawler does not crawl URLs which match one of the regular expressions provided.
      # The domain list contains the domains that cannot be crawled. Add to it if necessary.
      # The filetype list contains the file extensions that Orchestration Service does not support.
      # Remove from the regular expression filetypes you know are supported.
      # Ensure that your seed URL will not be excluded by a filter, or Crawler may hang.
      # Ensure that your seed URL domain is allowed by the filter.
      # Example: Use an empty filter for `allow everything` behaviour.
   
      urls_to_filter = [
        # domains and file extensions to disallow
        "(?i)\\.(xlsx?|pptx?|jpe?g|gif|png|mp3|tiff)$" 
      ],

      # max_text_size is the maximum size that a document can be before it is written to disk by the Connector Framework.
      # Adjusting this higher will decrease the amount of documents written to disk, but it will increase the memory requirement.
      # Add an appropriate setting for -Xmx in the jvm_arguments key in order to avoid out-of-memory scenarios.
      max_text_size = 1048576, #bytes

      # Add extra Java parameters (-Pkey=value) to the command used to launch the Connector Framework.
      extra_vm_params = {
        # Use Oakland libraries when available. Necessary for using the Sharepoint connector, no affect on others.
        "java.protocol.handler.pkgs" = "com.oaklandsw"
      },
      # Optionally log connector startup, useful for advanced debugging. Log file will be written to `crawler_temp_dir`.
      bootstrap_logging = false,
      # Sets how long the crawler should wait for a response from the connector framework
      read_timeout = 5 #seconds
    }
  },

output_adapter {
  # Select the Output Adapter to use by specifying its class. See crawler.conf(5) for more information or read below.
  class = "com.ibm.watson.crawler.discoveryserviceoutputadapter.DiscoveryServiceOutputAdapter",

  # Select the configuration key to be passed to the Output Adapter. The string here must correspond to a key within this configuration object.
  config = "discovery_service",

  # The Discovery Service Output Adapter uploads crawled documents to Watson Developer Cloud's Discovery Service.
  # Select it by setting
  #   class = "com.ibm.watson.crawler.discoveryserviceoutputadapter.DiscoveryServiceOutputAdapter",
  #   config = "discovery_service",
  discovery_service {
    include "discovery/discovery_service.conf"
  },

  # The Orchestration Service Output Adapter uploads crawled documents to Watson Developer Cloud's Orchestration Service.
  # Select it by setting
  #   class = "com.ibm.watson.crawler.orchestrationserviceoutputadapter.oneatatime.OrchestrationServiceOutputAdapter",
  #   config = "orchestration_service",
  orchestration_service {
    # Imports the contents of this file into this configuration key
    include "orchestration/orchestration_service.conf"
  },

  # The Test Output Adapter writes a representation of the crawled files to disk in the specified location.
  # To configure Data Crawler to use it, set
  #   class = "com.ibm.watson.crawler.testoutputadapter.TestOutputAdapter",
  #   config = "test",
  test {
    # Selects the directory to which a representation of the crawled data should be written.
    output_directory = "/tmp/crawler-test-output"
  },

  # Specifies the options for retry in case of failed attempts to push to the output adapter.
  # The formula is
  #
  #     d(nth_retry) = delay * (exponent_base ^ nth_retry)
  #
  # For example, the default settings with a delay of 1 second and an exponent base of 2
  # will cause the second retry - the third attempt - to delay 2 seconds instead of 1 and the next delay 4 seconds.
  #
  #     d(0) = 1 * (2 ^ 0) = 1 second
  #     d(1) = 1 * (2 ^ 1) = 2 seconds
  #     d(2) = 1 * (2 ^ 2) = 4 seconds
  #
  # So, with the default settings, a submission will be attempted up to 10 times waiting up to approximately 1022 seconds.
  # This time is "approximately" because there is additional time added to the time in order
  # to avoid having multiple resubmissions execute simultaneously. This fuzzed time is up to 10%, so the last retry in the
  # example above could delay up to 4.4 seconds.
  #
  # The wait time does not include the time spent connecting to the service, uploading data, or waiting for a response.
  # The throttling.output_timeout value takes precedence over the wait time here: if the total retry wait time exceeds
  # that setting, the submission will fail even if it should have been retried.
  #
  # Note that decreasing the wait time by lowering any of these defaults may result in documents not being successfully
  # uploaded via the configured output adapter. Evidence of this when using Watson Developer Cloud services will be
  # RetryFailure messages in the log citing "429 Too Many Requests" rate limiting.
  retry {
    # Maximum number of retry attempts.
    max_attempts = 10,
    
    # Minimum amount of delay between attempts, in seconds.
    delay = 2,

    # Factor that determines the growth of the delay time over each failed attempt.
    exponent_base = 2,
  }
},
# Debugging options
debugging {
  # Output the full data retrieved by the Crawler to the logs.
  # Warning: this will put the full data of every document crawled into the logs.
  full_node_debugging = false,
  # The time interval between reporting document ingestion statistics (Debugging mode only)
  # Units are measured in milliseconds
  heartbeat_wait_time = 1000
},
# Logging options
logging {
  # Log4j-specific options
  log4j {
    # The custom configuration used for this crawl, overrides the default configuration.
    # The default configuration is conveniently in this file already.
    configuration_file = "log4j_custom.properties"
  }
},
# Other crawler configuration that does not fit in elsewhere
miscellaneous {
  # Timeout to cleanup before terminating the application
  shutdown_timeout = 10 #seconds
},
#Defines sizing limitations for managing throughput
throttling {
  # Defines the upper limit of nodes that we will simultaneously *try* to send to the Output Adapter.
  # This will be further limited by cores available to do work.  It says "At any given point there will be no more than "X"
  # indexable items sent to the output adapter waiting to return.
  output_limit = 30,

  # Limits the number of urls requested from the input adapter at a time.
  # This is provided as a way to limit the number of new requests we send to the input adapter framework.
  input_limit = 30,

  # Represents how long crawler should wait, in seconds, before giving up on a request to the output adapter and remove the item
  # from the output adapter queue to allow more processing.
  #
  # This value takes precedence over any output adapter specific timing, including retry.
  #
  # This timeout should be at least 600 seconds when using the Orchestration Service Output Adapter because
  # that is the combined timeout of the services downstream from Orchestration, Document Conversion and Retrieve & Rank.
  output_timeout = 1200 #seconds

  threads {
    # The number of active threads in the core thread pool.
    # The value of this determines that number of active workers that deal with processing the incoming resource from
    # the input adapter and providing it to the output adapter.
    num_threads = 30
  }
}
